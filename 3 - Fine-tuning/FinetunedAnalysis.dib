#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"aliases":[],"languageName":"csharp","name":"csharp"}]}}

#!markdown

# Analysis of Fine-tuned Inferrence

After extracting data from the Asana API (see `FeedbackExtraction.dib`), using it to train a LLM (see `FinetuneTraining.dib`) and performing inferrence using the fine-tuned LLM (see `FinetuneInference.dib`) we're going to do some quick analysis to see well our fine-tuned LLM performed

We're want to plot this data on a chart so we're first going to import the Plotly library and bring it into scope.

#!csharp

#r "nuget: Plotly.NET.Interactive, 4.1.0"
#r "nuget: Plotly.NET.CSharp, 0.10.0"

using System.IO;
using Newtonsoft.Json;
using Plotly.NET.CSharp;

#!markdown

Next we'll load and deserialize the feedback categorized by our fine-tuned LLM ...

#!csharp

record FineTunedFeedbackItem(string Gid, string Notes, string FeedbackType, int FeedbackTypeIndex, int InferredFeedbackTypeIndex);

var json = await File.ReadAllTextAsync("./FinetunedInferrenceResults.json");
var feedbackItems = JsonConvert.DeserializeObject<FineTunedFeedbackItem[]>(json);

#!markdown

... and, for each type of feedback (i.e. "Something is broken", "Feature request", etc), calculate how often the fine-tuned LLM correctly classified the feedback.

#!csharp

var analysed = feedbackItems
    .GroupBy(feedbackItem => feedbackItem.FeedbackTypeIndex)
    .Select(group => (FeedbackTypeIndex: group.Key, Count: group.Count(), Correct: group.Where(feedback => feedback.InferredFeedbackTypeIndex == feedback.FeedbackTypeIndex).Count()))
    .Select(tuple => (FeedbackTypeIndex: tuple.FeedbackTypeIndex, Count: tuple.Count, Correct: tuple.Correct, CorrectPercent: ((double)tuple.Correct / (double)tuple.Count) * 100))
    .OrderBy(tuple => tuple.FeedbackTypeIndex)
    .ToArray();

#!markdown

Finally we can plot this data to a chart.

We'll use a stacked column chart, with one column per feedback type showing the percent of correct and incorrect categorization. We'll also add an "Overall" column showing how well our users do overall.

#!csharp

static readonly String[] GroupLabels = new []
{
    "Overall",
    "Something Is Broken", 
    "Feature Request", 
    "Usability",
    "Other"
};

Plotly.NET.CSharp.Chart
    .Combine(
        new []
        {
            Plotly.NET.CSharp.Chart.StackedColumn<double, string, string>(analysed.Select(tuple => tuple.CorrectPercent).ToArray().Prepend(analysed.Select(tuple => tuple.CorrectPercent).Average()), GroupLabels, "Correct %"),
            Plotly.NET.CSharp.Chart.StackedColumn<double, string, string>(analysed.Select(tuple => 100 - tuple.CorrectPercent).ToArray().Prepend(analysed.Select(tuple => 100 - tuple.CorrectPercent).Average()), GroupLabels, "Incorrect %")
        })
    .WithSize(Width: 1024, Height: 1024)
    .Display();

#!markdown

## Conclusion

From the above we can conclude the following:
* The fine-tuned LLM provide an incorrect classification around 32.8% of the time - 4.5% worse than user reported values.
* This incorrect classification value is heavily biased by the "Usability" category which is incorrect 100% of the time. Looking into the data, I've found that we only have a single feedback data point for this classification which is probably why we're unable to effectively classify it. By balancing our training data, we should be able to improve this.
* The second biggest category of misclassification is "Something Is Broken" with 8.4% of reports incorrect - 1.2% worse than user reported values.

Overall, users are fairly accurate in reporting feedback but a Windows engineer is still needed to triage the reports and correct the classification in many cases, particularly when the feedback relates to usability.
